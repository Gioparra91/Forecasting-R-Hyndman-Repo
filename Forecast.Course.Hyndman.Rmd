--- 
title: "FEE" 
output: pdf_document 
--- 

```{r packages and libraries} 
#install.packages("fpp2") 
rm(list=ls()) 
library(readxl) 
library(tseries) 
library(xts) 
library(lubridate) 
library(fpp2) 
``` 

```{r preprocessing} 
FEE <- read_excel(path = "globalMF.xlsx", sheet = "GMFR") 
FEE <- FEE[,2:149] 

FEEd <- apply(X = FEE, MARGIN = 2, FUN = diff) # diff 

dates <- seq.Date(ymd("2008-12-31"), length.out = 10, by="year") 

rownames(FEEd) <- as.character(dates) 
FEEd <- as.xts(FEEd) 

#FEE <- data.frame(apply(X = FEE, MARGIN = 2, FUN = function(x){ts(x, frequency = 1, start = 2007)})) # ts 

plot(FEEd$G.RpI.MFpE.ApP., type="l", xlab="", ylab="") 
title(main="This is an example") 
``` 

# **1 Time Series Introduction** 

```{r 1 ts} 
'1 INTRO' 
# create ts 
FEE$G.RpI.MFpE.ApP. <- ts(x, start = 2008, frequency = 1) #frequency 1, 4, 12, 52 
``` 

# 2 GRAPHICS 
## ts patterns 
autplot ts 

## seasonal plots 

## Scatter plots 

## Lag, Autocorr and WN 

```{r 2 GRAPHICS} 

autoplot(x)+ggtitle("autoplot test")+xlab("")+ylab("") 

'seasonal plots' 
#seasonal plot: A seasonal plot allows the underlying seasonal pattern to be seen more clearly, and is especially useful in identifying years in which the pattern changes. 
ggseasonplot(FEE$G.RpI.MFpE.ApP., year.labels=T, year.labels.left=T)+ggtitle("seaonalplot") 

#seasonalplot polar: to see montly variation is useful (supports mts,ts and matrix) 
ggseasonplot(x, polar=TRUE)+ggtitle("Polar seasonal plot") 

#seasonalsubseries plot: shows the changes in seasonality over time. It is especially useful in identifying changes within particular seasons. 
ggsubseriesplot(x)+ggtitle("Seasonal subseries plot") 

'scatterplot' 
#scatterplot and correlation: plot all predictors 
autoplot(visnights[,2:5], facets=T)+ylab("Faceting to compare patterns") 

#correlation graph 
FEE[,1:5] %>% as.data.frame() %>% GGally::ggpairs() #if not aleady DF 

'lag, autocorr and WN' 
# lag plot 
gglagplot(FEE$G....) 

#AUTOCORRELATION 
ggAcf(FEE$G....) #bans are +-2*sqrt(T) where T= no. obs 

#whitenoise 
set.seed(30) 
y <- ts(rnorm(50)) 
autoplot(y) 
``` 

# **3 FORECASTER TOOLBOX** 

## population adjustments: Any data that are affected by population changes can be adjusted to give per-capita data. That is, consider the data per person (or per thousand people, or per million people) rather than the total. 
  
## inflation adjustments: financial time series are usually adjusted so that all values are stated in dollar values from a particular year. For example, the house price data may be stated in year 2000 dollars. To make these adjustments, a price index is used. If zt denotes the price index and yt denotes the original house price in year t, then xt=yt/zt∗z2000 gives the adjusted house price at year 2000 dollar values. Price indexes are often constructed by government agencies. For consumer goods, a common price index is the Consumer Price Index (or CPI). 
  
## mathematical transformation: If the data show variation that increases or decreases with the level of the series, then a transformation can be useful. For example, a logarithmic transformation is often useful. Logarithms are useful because they are interpretable: changes in a log value are relative (or percentage) changes on the original scale. So if log base 10 is used, then an increase of 1 on the log scale corresponds to a multiplication of 10 on the original scale. 
  
## BIAS ADJUSTMENT: The difference between the simple back-transformed forecast given by the blue line and the mean given by the red line is called the bias. When we use the mean, rather than the median, we say the point forecasts have been bias-adjusted. 

## Residual fit diagnostic 

Fitted values definition: Each observation in a time series can be forecast using all previous observations. Actually, fitted values are often not true forecasts because any parameters involved in the forecasting method are estimated using all available observations in the time series 

Residual: The “residuals” in a time series model are what is left over after fitting a model. Residuals are useful in checking whether a model has adequately captured the information in the data. 1 The residuals are uncorrelated; 2 The residuals have zero mean. 

Example:how to read them acf and hist of residual plot: These graphs show that the naïve method produces forecasts that appear to account for all available information. The mean of the residuals is close to zero and there is no significant correlation in the residuals series. The time plot of the residuals shows that the variation of the residuals stays much the same across the historical data, apart from the one outlier, and therefore the residual variance can be treated as constant. This can also be seen on the histogram of the residuals. The histogram suggests that the residuals may not be normal — the right tail seems a little too long, even when we ignore the outlier. Consequently, forecasts from this method will probably be quite good, but prediction intervals that are computed assuming a normal distribution may be inaccurate. 

The residuals obtained from forecasting google series using the naïve method are shown in Figure above. The large positive residual is a result of the unexpected price jump at day 166. 

## portmanteau tests for autocorrelation: In addition to looking at the ACF plot, we can also do a more formal test for autocorrelation by considering a whole set of rk values as a group, rather than treating each one separately. Recall that rk is the autocorrelation for lag k. 

## training and test dataset 
Some references describe the test set as the “hold-out set” because these data are “held out” of the data used for fitting. Other references call the training set the “in-sample data” and the test set the “out-of-sample data”. We prefer to use “training data” and “test data” in this book. 

## evaluation 
MAE: mean(abs(e_{t})) 
RMSE: sqrt(mean(e**2_{t}) 
MAPE: p_{t}=100e_{t}/y_{t} and MAPE is mean(abs(p_{t})) 

evaluation using pipes(this does the same than above): The left hand side of each pipe is passed as the first argument to the function on the right hand side. This is consistent with the way we read from left to right in English. When using pipes, all other arguments must be named, which also helps readability. When using pipes, it is natural to use the right arrow assignment -> rather than the left arrow. For example, the third line above can be read as “Take the goog200 series, pass it to rwf() with drift=TRUE, compute the resulting residuals, and store them as res”. 

## predicting interval 
When plotted, the prediction intervals are shown as shaded region, with the strength of colour indicating the probability associated with the interval. Then we can compute prediction intervals by calculating percentiles for each forecast horizon. The result is called a bootstrapped prediction interval. 

```{r 3 FORECASTER TOOLBOX} 
'simple forecast methods' 
# AVERAGE 
meanf(FEE$G...., h=8) 

# NAIVE METHOD AND RANDOMWALK 
naive(FEE$G...., h=8) 
rwf(FEE$G...., h=8) 

#SEASONAL NAIVE METHOD 
snaive(FEE$G...., h=8) 

# RANDOM WALK WITH DRIFT 
rwf(FEE$G...., h=8, drift = T) 

#EXAMPLES 
ts <- ts(FEE$G...., frequency = 1, start = 2008) 
autoplot(ts) + 
  autolayer(meanf(ts, h=8),series="Mean", PI=FALSE) + 
  autolayer(naive(ts, h=8),series="Naïve", PI=FALSE) + 
  autolayer(snaive(ts, h=8),series="Seasonal naïve", PI=FALSE) + 
  ggtitle("Forecasts") + 
  guides(colour=guide_legend(title="Forecast")) 

autoplot(goog200) + 
  autolayer(meanf(goog200, h=40),series="Mean", PI=FALSE) + 
  autolayer(rwf(goog200, h=40),series="Naïve", PI=FALSE) + 
  autolayer(rwf(goog200, drift=TRUE, h=40),series="Drift", PI=FALSE) + 
  ggtitle("Google stock (daily ending 6 Dec 2013)") + 
  guides(colour=guide_legend(title="Forecast")) 

class(ts) 

'transformation and adjustments' 
#calendar adjustments 
dframe <- cbind(Monthly = milk, 
                DailyAverage = milk/monthdays(milk)) 
  autoplot(dframe, facet=TRUE) + 
    xlab("Years") + ylab("Pounds") + 
    ggtitle("Milk production per cow") 
  

ts <- ts(FEE$G...., frequency = 1, start = 2008)   

fc <- rwf(ts, drift=T, lambda=0.1, h=8, level=80) # lambda is log transformation 
fc2 <- rwf(ts, drift=T, lambda=0.1, h=8, level=80, biasadj=TRUE) 
autoplot(ts) + 
  autolayer(fc, series="Simple back transformation") + 
  autolayer(fc2, series="Bias adjusted", PI=FALSE) + 
  guides(colour=guide_legend(title="Forecast")) 

'residual diagnostic' 
autoplot(goog200) + 
  xlab("Day") + ylab("Closing Price (US$)") + 
  ggtitle("Google Stock (daily ending 6 December 2013)") 
res <- residuals(naive(goog200)) 
autoplot(res) + xlab("Day") + ylab("") + 
  ggtitle("Residuals from naïve method") 

gghistogram(res)+ggtitle("Hist of Residuals") 
ggAcf(res)+ggtitle("ACF of Residuals") 

checkresiduals(naive(goog200)) # gives all the test you need. For both Q and Q∗, the results are not significant (i.e., the p-values are relatively large). Thus, we can conclude that the residuals are not distinguishable from a white noise series. 

'evaluating forecast accuracy' 
window(ausbeer, start=1995) #all data in 1995 
subset(ausbeer, start=length(ausbeer)-4*5) # more types of subsetting 
tail(ausbeer, 4*5) # last obs 
#===> use caret package!and take random 20% of data. 
e <- tsCV(goog200, rwf, drift=TRUE, h=1) 
sqrt(mean(e^2, na.rm=TRUE)) 
sqrt(mean(residuals(rwf(goog200, drift=TRUE))^2, na.rm=TRUE)) 

beer2 <- window(ausbeer,start=1992,end=c(2007,4)) 
beerfit2 <- rwf(beer2,h=10) 
beer3 <- window(ausbeer, start=2008) 
accuracy(beerfit2, beer3) 

goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e 
e^2 %>% mean(na.rm=TRUE) %>% sqrt() 
goog200 %>% rwf(drift=TRUE) %>% residuals() -> res 
res^2 %>% mean(na.rm=TRUE) %>% sqrt() 

'prediction interval' 
autoplot(naive(goog200)) 
autoplot(naive(goog200, bootstrap=TRUE)) 
``` 


# **4 JUDGMENTAL FORECAST** 
## introduction 
There are three general settings in which judgmental forecasting is used: (i) there are no available data, so that statistical methods are not applicable and judgmental forecasting is the only feasible approach; (ii) data are available, statistical forecasts are generated, and these are then adjusted using judgement; and (iii) data are available and statistical and judgmental forecasts are generated independently and then combined. 

## key principles 
produce an organigram of the factors and their relations through arrows. that guidelines for forecasting new policy impacts be developed, to encourage a more systematic and structured forecasting approach; that the forecast methodology be documented in each case, including all assumptions made in forming the forecasts; that new policy forecasts be made by at least two people from different areas of the organisation; that a review of forecasts be conducted one year after the implementation of each new policy by a review committee, especially for new policies that have a significant annual projected cost or saving. The review committee should include those involved in generating the forecasts, but also others. 

## delhi method 
The aim of the Delphi method is to construct consensus forecasts from a group of experts in a structured iterative manner. 

## scenario forecasting 
assess forecasts based on qualitative given probabilities: best- middle- worst case scenario. 

# **5 TIME SERIES REGRESSION MODELS** 

## Simple Linear Regression 
$y_{t}=\beta_{0}+\beta_{1}x_{t}+\epsilon_{t}$ 
```{r} 
global <- ts(FEEd$G...., frequency = 1, start = 2008) 
passive <- ts(FEEd$G.RpI.MFpE.Passive., frequency = 1, start = 2008) 

autoplot(global)+ggtitle("example: Are global fees driven by passive?")+ 
  autolayer(passive) 
``` 

Scatterplot and Simple Linear Model 

```{r} 
DF <- data.frame(global, passive) 
colnames(DF) <- c("Global", "Passive") 
DF %>% 
  ggplot(aes(x=Passive, y=Global))+ 
  xlab("Passive")+ylab("Global")+ 
  geom_point()+ 
  geom_smooth(method="lm", se=T) 
``` 

Estimating the equation in R for ts using tslm() or for vectors using lm(). 
The fitted line has a positive slope, reflecting the positive relationship between globa and passive. The slope coefficient shows that a one unit increase in x results on average in 1.42 units increase in y. Alternatively the estimated equation shows that a value of 1 for x will result in a forecast value of  -0.002+1.426×1 for y. 
The interpretation of the intercept requires that a value of  x=0 makes sense. In this case when  x=0 (i.e., when passive fees are zero) the predicted value of y is ~0. Even when  x=0 does not make sense, the intercept is an important part of the model. Without it, the slope coefficient can be distorted unnecessarily. The intercept should always be included unless the requirement is to force the regression line “through the origin”. In what follows we assume that an intercept is always included in the model. 

```{r} 
reg <- tslm(y~x, data=DF) 
reg 
``` 

Multiple Linear Regression 
More than one regressor variables. You can facet the plot horizontally to see their patterns simultaneusly. Use GGgally to show their relative correlation, distribution and scatterplots simultaneously. 

Assumptions of Linear Models 
1 they have mean zero; otherwise the forecasts will be systematically biased. 
2 they are not autocorrelated; otherwise the forecasts will be inefficient, as there is more information in the data that can be exploited. 
3 they are unrelated to the predictor variables; otherwise there would be more information that should be included in the systematic part of the model. 

## Least Squares Estimation 
When you run tslm() or lm() you estimate the parameters $\beta$ that minimize the error. The summary of the regression gives all you need to know. 
```{r} 
summary(reg) 
``` 

Plotting the fitted values give you a hind on how your modelperforms in sample. Goodness of fit is given by $R^2$ in the Summary of Regression 
```{r} 
autoplot(y, series="Data") + 
  autolayer(fitted(reg), series="Fitted") + 
  xlab("Year") + ylab("") + 
  ggtitle("Passive-regressor vs real data") + 
  guides(colour=guide_legend(title=" ")) 

cbind(Data = y, 
      Fitted = fitted(reg)) %>% 
  as.data.frame() %>% 
  ggplot(aes(x=Data, y=Fitted)) + 
    geom_point() + 
    xlab("Fitted (predicted values)") + 
    ylab("Data (actual values)") + 
    ggtitle("Passive-regressor vs real data") + 
    geom_abline(intercept=0, slope=1) 
``` 

## Model evaluation using checkresiduals() 
The time plot shows some changing variation over time, but is otherwise relatively unremarkable. This heteroscedasticity will potentially make the prediction interval coverage inaccurate. 

The histogram shows that the residuals seem to be slightly skewed, which may also affect the coverage probability of the prediction intervals. 

The autocorrelation plot shows a significant spike at lag 7, but it is not quite enough for the Breusch-Godfrey to be significant at the 5% level. In any case, the autocorrelation is not particularly large, and at lag 7 it is unlikely to have any noticeable impact on the forecasts or the prediction intervals. In Chapter 9 we discuss dynamic regression models used for better capturing information left in the residuals. 

We would also expect the residuals to be randomly scattered without showing any systematic patterns. If a pattern is observed, there may be “heteroscedasticity” in the errors which means that the variance of the residuals may not be constant. If this problem occurs, a transformation of the forecast variable such as a logarithm or square root may be required. If no patterns appear, they are Homoskedastic. 

```{r} 
checkresiduals(reg) 

cbind(Fitted = fitted(reg), 
      Residuals=residuals(reg)) %>% 
  as.data.frame() %>% 
  ggplot(aes(x=Fitted, y=Residuals)) + geom_point() 
``` 

Evaluate whether to remove outliers (when u have many observations, not in this case) 

Spurious regression: when the series are not stationary. that is, the values of the time series do not fluctuate around a constant mean or with a constant variance. High R2 and high residual autocorrelation can be signs of spurious regression. The example below shows air passenger and rice price. 

```{r} 
aussies <- window(ausair, end=2011) 
fit <- tslm(aussies ~ guinearice) 
summary(fit) 
checkresiduals(fit) 
``` 

## Some useful predictors in tslm() 
1 Trend. see more later 
2 Dummy variables for categorical variables. 
3 season 

```{r} 
reg2 <- tslm(y~trend+x) 
summary(reg2) 

autoplot(y, series="Data")+ 
  autolayer(fitted(reg2), series="fitter") 
``` 

Example with ausbeer dataset: create a window of the ts. fit a tslm() model. Using pipeline cbind the data and the fitted in a dataset and plot the result coloring by the quarterly cycles of the beer 
```{r} 
beer2 <- window(ausbeer, start=1992) 
fit.beer <- tslm(beer2 ~ trend + season) 
cbind(Data=beer2, Fitted=fitted(fit.beer)) %>% 
  as.data.frame() %>% 
  ggplot(aes(x = Data, y = Fitted, 
             colour = as.factor(cycle(beer2)))) + 
    geom_point() + 
    ylab("Fitted") + xlab("Actual values") + 
    ggtitle("Quarterly beer production") + 
    scale_colour_brewer(palette="Dark2", name="Quarter") + 
    geom_abline(intercept=0, slope=1) 

``` 

## Selecting predictors 
When there are many possible predictors, we need some strategy for selecting the best predictors to use in a regression model. 

Not recommended approaches: 
1 A common approach that is not recommended is to plot the forecast variable against a particular predictor and if there is no noticeable relationship, drop that predictor from the model. This is invalid because it is not always possible to see the relationship from a scatterplot, especially when the effects of other predictors have not been accounted for. 
2 Another common approach which is also invalid is to do a multiple linear regression on all the predictors and disregard all variables whose   
p-values are greater than 0.05. 

Recommended: use the following parameters to evaluate the accuracy by CV() function 
```{r} 
CV(reg2) 
``` 

## Forecasting with regression 
predictions of y can be obtained using the formula which comprises the estimated coefficients and ignores the error in the regression equation. Plugging in the values of the predictor variables returned the fitted (training-sample) values of y. What we are interested in here, however, is forecasting future values of y. 

Ex-ante forecasts are those that are made using only the information that is available in advance. For example, ex-ante forecasts for the percentage change in US consumption for quarters following the end of the sample, should only use information that was available up to and including 2016 Q3. These are genuine forecasts, made in advance using whatever information is available at the time. Therefore in order to generate ex-ante forecasts, the model requires forecasts of the predictors. To obtain these we can use one of the simple methods introduced before or Gov Agencies forecasts. 

Ex-post forecasts are those that are made using later information on the predictors. For example, ex-post forecasts of consumption may use the actual observations of the predictors, once these have been observed. These are not genuine forecasts, but are useful for studying the behaviour of forecasting models. 

## Scenario based forecasting 
In this setting, the forecaster assumes possible scenarios for the predictor variables that are of interest. For example, a US policy maker may be interested in comparing the predicted change in consumption when there is a constant growth of 1% and 0.5% respectively for income and savings with no change in the employment rate, versus a respective decline of 1% and 0.5%, for each of the four quarters following the end of the sample. 

Assuming that for the next four quarters, personal income will increase by its historical mean value of x=0.72%, consumption is forecast to increase by  1.95%and the corresponding  95% 
and  80%prediction intervals are[0.69,3.21]and [1.13,2.77]respectively (calculated using R). If we assume an extreme increase of  5% in income, then the prediction intervals are considerably wider as shown in Figur 
  
```{r} 
#up and downs 
fit.consBest <- tslm( 
  Consumption ~ Income + Savings + Unemployment, 
  data = uschange) 
h <- 4 
newdata <- data.frame( 
    Income = c(1, 1, 1, 1), 
    Savings = c(0.5, 0.5, 0.5, 0.5), 
    Unemployment = c(0, 0, 0, 0)) 
fcast.up <- forecast(fit.consBest, newdata = newdata) #use forecast function to fill with the new data 
newdata <- data.frame( 
    Income = rep(-1, h), 
    Savings = rep(-0.5, h), 
    Unemployment = rep(0, h)) 
fcast.down <- forecast(fit.consBest, newdata = newdata) 

autoplot(uschange[, 1]) + 
  ylab("% change in US consumption") + 
  autolayer(fcast.up, PI = TRUE, series = "increase") + 
  autolayer(fcast.down, PI = TRUE, series = "decrease") + 
  guides(colour = guide_legend(title = "Scenario")) 

# etreme scenario 
fit.cons <- tslm(Consumption ~ Income, data = uschange) 
h <- 4 
fcast.ave <- forecast(fit.cons, 
  newdata = data.frame( 
    Income = rep(mean(uschange[,"Income"]), h))) 
fcast.up <- forecast(fit.cons, 
  newdata = data.frame(Income = rep(5, h))) 
autoplot(uschange[, "Consumption"]) + 
  ylab("% change in US consumption") + 
  autolayer(fcast.ave, series = "Average increase", 
    PI = TRUE) + 
  autolayer(fcast.up, series = "Extreme increase", 
    PI = TRUE) + 
  guides(colour = guide_legend(title = "Scenario")) 
``` 

```{r} 
global <- ts(FEEd$G...., frequency = 1, start = 2008) 
passive <- ts(FEEd$G.RpI.MFpE.Passive., frequency = 1, start = 2008) 
DF <- data.frame(global, passive) 
colnames(DF) <- c("Global", "Passive") 

reg <- lm(DF$Global ~ DF$Passive, data=DF) 

fit.cons <- tslm(Consumption ~ Income, data = uschange) 
h <- 4 
fcast.ave <- forecast(fit.cons, 
  newdata = data.frame( 
    Income = rep(mean(uschange[,"Income"]), h))) 
fcast.up <- forecast(fit.cons, 
  newdata = data.frame(Income = rep(5, h))) 
autoplot(uschange[, "Consumption"]) + 
  ylab("% change in US consumption") + 
  autolayer(fcast.ave, series = "Average increase", 
    PI = TRUE) + 
  autolayer(fcast.up, series = "Extreme increase", 
    PI = TRUE) + 
  guides(colour = guide_legend(title = "Scenario")) 
``` 

## Non Linear Regression (log-log) 
While this provides a non-linear functional form, the model is still linear in the parameters. Recall that in order to perform a logarithmic transformation to a variable, all of its observed values must be greater than zero.  In the case that variable x contains zeros, we use the transformation log(x+1) ; i.e., we add one to the value of the variable and then take logarithms. This has a similar effect to taking logarithms but avoids the problem of zeros. It also has the neat side-effect of zeros on the original scale remaining zeros on the transformed scale. 

```{r} 
h <- 10 
fit.lin <- tslm(marathon ~ trend) #linear trend 
fcasts.lin <- forecast(fit.lin, h = h) 
fit.exp <- tslm(marathon ~ trend, lambda = 0) # exponential 
fcasts.exp <- forecast(fit.exp, h = h) 

t <- time(marathon) 
t.break1 <- 1940 
t.break2 <- 1980 
tb1 <- ts(pmax(0, t - t.break1), start = 1897) 
tb2 <- ts(pmax(0, t - t.break2), start = 1897) 

fit.pw <- tslm(marathon ~ t + tb1 + tb2) # piecewise 
t.new <- t[length(t)] + seq(h) 
tb1.new <- tb1[length(tb1)] + seq(h) 
tb2.new <- tb2[length(tb2)] + seq(h) 

newdata <- cbind(t=t.new, tb1=tb1.new, tb2=tb2.new) %>% 
  as.data.frame() 
fcasts.pw <- forecast(fit.pw, newdata = newdata) 

fit.spline <- tslm(marathon ~ t + I(t^2) + I(t^3) + # spline 
  I(tb1^3) + I(tb2^3)) 
fcasts.spl <- forecast(fit.spline, newdata = newdata) 

autoplot(marathon) + 
  autolayer(fitted(fit.lin), series = "Linear") + 
  autolayer(fitted(fit.exp), series = "Exponential") + 
  autolayer(fitted(fit.pw), series = "Piecewise") + 
  autolayer(fitted(fit.spline), series = "Cubic Spline") + 
  autolayer(fcasts.pw, series="Piecewise") + 
  autolayer(fcasts.lin, series="Linear", PI=FALSE) + 
  autolayer(fcasts.exp, series="Exponential", PI=FALSE) + 
  autolayer(fcasts.spl, series="Cubic Spline", PI=FALSE) + 
  xlab("Year") + ylab("Winning times in minutes") + 
  ggtitle("Boston Marathon") + 
  guides(colour = guide_legend(title = " ")) 
``` 

```{r exp smoothing} 
GlobalFEE=ts(FEE$G...., frequency=1, start=2007) 

GlobalFEE%>% 
  splinef(lambda=0) %>% 
  autoplot()+ 
  ylim(0,1) 

smoothFEE <- splinef(GlobalFEE, h=8, lambda=0.1, biasadj=TRUE,method="mle") 

autoplot(smoothFEE)+ggtitle("Cubic Smoothing Spline Global Fees")+xlab("")+ylab("") 

summary(smoothFEE) 

smoothFEE %>% checkresiduals() 
``` 

# ** 6 Time Series Decomposition** 

## Moving Averages 
```{r} 
GlobalFEE <- ts(FEE$G...., frequency=1, start=2007) 

ma3 <- ma(GlobalFEE, 3) # no values for the first and last because we take the avg of 3 

autoplot(GlobalFEE, series="Data")+ 
  autolayer(ma3, series="3-MA")+ 
  ggtitle("Global Fees")+xlab("")+ylab("") 
``` 

## Classical Decomposition 
While classical decomposition is still widely used, it is not recommended, as there are now several much better methods. 
```{r} 
elecequip %>% decompose(type="multiplicative") %>% 
  autoplot() + xlab("Year") + 
  ggtitle("Classical multiplicative decomposition 
    of electrical equipment index") 
``` 

## X11 Decomposition 

## SEAT 

## STL decomposition 
STL is a versatile and robust method for decomposing time series. STL is an acronym for “Seasonal and Trend decomposition using Loess”, while Loess is a method for estimating nonlinear relationships. 
STL has several advantages over the classical, SEATS and X11 decomposition methods: 
  1. Unlike SEATS and X11, STL will handle any type of seasonality, not only monthly and quarterly data. 
  2. The seasonal component is allowed to change over time, and the rate of change can be controlled by the user. 
  3. The smoothness of the trend-cycle can also be controlled by the user. 
4. It can be robust to outliers (i.e., the user can specify a robust decomposition), so that occasional unusual observations will not affect the estimates of the trend-cycle and seasonal components. They will, however, affect the remainder component. 

On the other hand, STL has some disadvantages. In particular, it does not handle trading day or calendar variation automatically, and it only provides facilities for additive decompositions. It is possible to obtain a multiplicative decomposition by first taking logs of the data, then back-transforming the components. Decompositions between additive and multiplicative can be obtained using a Box-Cox transformation of the data with 0<λ<1. A value of  λ=0 corresponds to the multiplicative decomposition while  λ=1 
is equivalent to an additive decomposition. 

The two main parameters to be chosen when using STL are the trend-cycle window (t.window) and the seasonal window (s.window). These control how rapidly the trend-cycle and seasonal components can change. The mstl() function provides a convenient automated STL decomposition using s.window=13, and t.window also chosen automatically. 

```{r} 
elecequip %>% 
  stl(t.window=13, s.window="periodic", robust=TRUE) %>% 
  autoplot() 
``` 

## Forecasting with seasonal adjusted data 
While decomposition is primarily useful for studying time series data, and exploring historical changes over time, it can also be used in forecasting. 

Assuming an additive decomposition, the decomposed time series can be written as $y_{t}=S^{hat}_{t}+A^{hat}_{t}$,where  $A^{h}_{t}=T^{h}_{t}+R^{h}_{t}$ is the seasonally adjusted component. Or, if a multiplicative decomposition has been used, we can write $y_{t}=S^{hat}_{t}A^{hat}_{t}$,where  $A^{hat}_{t}=T^{hat}_{t}R^{hat}_{t}$.To forecast a decomposed time series, we forecast the seasonal component,  ^St, and the seasonally adjusted component ^At, separately. It is usually assumed that the seasonal component is unchanging, or changing extremely slowly, so it is forecast by simply taking the last year of the estimated component. In other words, a seasonal naïve method is used for the seasonal component. 

To forecast the seasonally adjusted component, any non-seasonal forecasting method may be used. For example, a random walk with drift model, or Holt’s method, or a non-seasonal ARIMA model, may be used. 

The first figure shows naïve forecasts of the seasonally adjusted electrical equipment orders data. These are then “reseasonalized” by adding in the seasonal naïve forecasts of the seasonal component. 
```{r} 
fit <- stl(elecequip, t.window=13, s.window="periodic", 
  robust=TRUE) 
fit %>% seasadj() %>% naive() %>% 
  autoplot() + ylab("New orders index") + 
  ggtitle("Naive forecasts of seasonally adjusted data") 
``` 

This is made easy with the forecast() function applied to the stl object. You need to specify the method being used on the seasonally adjusted data, and the function will do the reseasonalizing for you. The resulting forecasts of the original data are shown in Figure. 

```{r} 
fit %>% forecast(method="naive") %>% # reseasonalization automatic if you specify the method used 
  autoplot() + ylab("New orders index") 
``` 


A short-cut approach is to use the stlf() function. The following code will decompose the time series using STL, forecast the seasonally adjusted series, and return reseasonalize the forecasts 
```{r} 
stlf(elecequip, method='naive') %>% autoplot() + ylab("New orders index") 
``` 


# **7 Exponential Smoothing (ses() function)** 

Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in industry. 

## Simple exponential smoothing 
This method is suitable for forecasting data with no clear trend or seasonal pattern. 

```{r} 
oildata <- window(oil, start=1996) 
autoplot(oildata) + 
  ylab("Oil (millions of tonnes)") + xlab("Year") 
``` 

We have already considered the naïve and the average as possible methods for forecasting such data. Using the naïve method, all forecasts for the future are equal to the last observed value of the series. Hence, the naïve method assumes that the most recent observation is the only important one, and all previous observations provide no information for the future. This can be thought of as a weighted average where all of the weight is given to the last observation. (formula) 

Using the average method, all future forecasts are equal to a simple average of the observed data. Hence, the average method assumes that all observations are of equal importance, and gives them equal weights when generating forecasts. (formula) 

it may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages, where the weights decrease exponentially as observations come from further in the past — the smallest weights are associated with the oldest observations. The higher $\alpha$ the higher the importance of the most recent observations. $\alpha =1$ is the naive forecast. 
$y^{hat}_{t+1|t}=\alpha y+{T}+\alpha (1-\alpha) y_{T-1}+\alpha (1-\alpha)^2 y_{T-2}+... $ 


```{r} 
GlobalFEE <- ts(FEE$G...., frequency=1, start=2007) 

fc1 <- ses(GlobalFEE, h=8) 
round(accuracy(fc1),2) 

autoplot(fc1) + 
  autolayer(fitted(fc1), series="Fitted") + 
  ylab("") + xlab("") 
``` 

## Holt winter 

Forecast equation $y^{hat}_{t+h|t}=l_{t}+hb_{t}$ 
Level equation $l_{t}=\alpha y_{t}+(1-\alpha)(l_{t-1}+b_{t})$ 
Trend Equation $b_{t}=\beta^*(l_{t}+l_{t-1})+(1-\beta^*)b_{t-1}$ 

Example on airpassenger data and α=0.8321 and β∗=0.0001 

```{r} 
air <- window(ausair, start=1990) 
fc <- holt(air, h=5) 
fc 
``` 


The forecasts generated by Holt’s linear method display a constant trend (increasing or decreasing) indefinitely into the future. Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons. Methods that include a damped trend have proven to be very successful, and are arguably the most popular individual methods when forecasts are required automatically for many series. 

In conjunction with the smoothing parameters α and β∗ (with values between 0 and 1 as in Holt’s method), this method also includes a damping parameter 0<ϕ<1. If  ϕ=1, the method is identical to Holt’s linear method. For values between  0 and  1, ϕ dampens the trend so that it approaches a constant some time in the future. 

```{r} 
fc3 <- holt(GlobalFEE, h=8) 
fc4 <- holt(GlobalFEE, damped=TRUE, phi = 0.9, h=8) 
autoplot(GlobalFEE) + 
  autolayer(fc3, series="Holt's method", PI=FALSE) + 
  autolayer(fc4, series="Damped Holt's method", PI=FALSE) + 
  ggtitle("Forecasts from Holt's method") + xlab("") + 
  ylab("") + 
  guides(colour=guide_legend(title="Forecast")) 

``` 

## compare exponential forecast 
ses, hold and hold damped. ses looks better even though he says damped is the best usually 
```{r} 
e1 <- tsCV(GlobalFEE, ses, h=8) 
e2 <- tsCV(GlobalFEE, holt, h=8) 
e3 <- tsCV(GlobalFEE, holt, damped=TRUE, h=8) 

mean(e1^2, na.rm=TRUE); mean(e2^2, na.rm=TRUE); mean(e3^2, na.rm=TRUE) 
mean(abs(e1), na.rm=TRUE); mean(abs(e2), na.rm=TRUE); mean(abs(e3), na.rm=TRUE) 

fc1 <- ses(GlobalFEE, h=8) 
fc4 <- holt(GlobalFEE, damped=TRUE, phi = 0.9, h=8) 

fc1[["model"]] 
fc4[["model"]] # damped looks better 
``` 

```{r} 
autoplot(fc4)+ggtitle("Forecasts from Damped Holt's method") + xlab("") + 
  ylab("") 
``` 

## Hold Winter seasonal method 

A method that often provides accurate and robust forecasts for seasonal data is the Holt-Winters method with a damped trend and multiplicative seasonality 

```{r} 
fc <- hw(subset(hyndsight,end=length(hyndsight)-35), 
         damped = TRUE, seasonal="multiplicative", h=35) 
autoplot(hyndsight) + 
  autolayer(fc, series="HW multi damped", PI=FALSE)+ 
  guides(colour=guide_legend(title="Daily forecasts")) 
``` 

## Innovations state space models for exponential smoothing 

A statistical model is a stochastic (or random) data generating process that can produce an entire forecast distribution. We will also describe how to using the model selection criteria introduced in Chapter 5 to choose the model in an objective manner. 

Each model consists of a measurement equation that describes the observed data, and some state equations that describe how the unobserved components or states (level, trend, seasonal) change over time. Hence, these are referred to as state space models. 

For each method there exist two models: one with additive errors and one with multiplicative errors. The point forecasts produced by the models are identical if they use the same smoothing parameter values. They will, however, generate different prediction intervals. 

To distinguish between a model with additive errors and one with multiplicative errors (and also to distinguish the models from the methods), we add a third letter to the classification of Table 7.5. We label each state space model as ETS(.,.,.) for (Error, Trend, Seasonal). 


## Estimation and model selection 

An alternative to estimating the parameters by minimizing the sum of squared errors is to maximize the “likelihood”. The likelihood is the probability of the data arising from the specified model. Thus, a large likelihood is associated with a good model. For an additive error model, maximizing the likelihood gives the same results as minimizing the sum of squared errors. However, different results will be obtained for multiplicative error models. In this section, we will estimate the smoothing parameters   
α,β,γ and  ϕ, and the initial states  ℓ0,  b0,  s0,s−1,…,s−m+1, by maximizing the likelihood. 

The possible values that the smoothing parameters can take are restricted. 
the traditional restrictions translate to   
0<α<1,  0<β<α and  0<γ<1−α. In practice, the damping parameter  ϕ is usually constrained further to prevent numerical difficulties in estimating the model. In R, it is restricted so that  0.8<ϕ<0.98. 

Model selection By information criteria. The models can be estimated in R using the ets() function in the forecast package. Unlike the ses(), holt() and hw() functions, the ets() function does not produce forecasts. Rather, it estimates the model parameters and returns information about the fitted model. By default it uses the AICc to select an appropriate model, although other information criteria can be selected. 

ets(y, model="ZZZ", damped=NULL, alpha=NULL, beta=NULL, 
    gamma=NULL, phi=NULL, lambda=NULL, biasadj=FALSE, 
    additive.only=FALSE, restrict=TRUE, 
    allow.multiplicative.trend=FALSE) 

Estimate the ETS among all the exponential smoothing models within the state space models. This function estimate the parameters but doesnt foreacst.     
```{r} 
GlobalFEE <- ts(FEE$G...., frequency=1, start=2007) 

fit <- ets(GlobalFEE) 
summary(fit) # first ouput 

autoplot(fit) # second output 

cbind('Residuals' = residuals(fit), #third output 
      'Forecast errors' = residuals(fit,type='response')) %>% 
  autoplot(facet=TRUE) + xlab("Year") + ylab("") 
``` 

## Forecasting using ETS 

```{r} 
fit %>% 
  forecast(h=8, bootstrap=TRUE, biasadjust=TRUE) %>% 
  autoplot()+ggtitle("ETS result forecast") 
``` 

# ** ARIMA models  ** 

## Stationarity and differencing 
It's a covariance stationary process or a process where the first two moments are constant. In simple words, A stationary time series is one whose properties do not depend on the time at which the series is observed. 

1 Logarithm: Transformations such as logarithms can help to stabilize the variance of a time series. 
2 Differencing: Differencing can help stabilize the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality. **If you have a price and you compute the difference, The differenced series is the change between consecutive observations in the original series**. 

Example: The ACF of the differenced Google stock price looks just like that of a white noise series. There are no autocorrelations lying outside the 95% limits, and the Ljung-Box  Q∗ statistic has a p-value of 0.355 (for   
h=10). This suggests that the **daily change** in the Google stock price is essentially a random amount which is uncorrelated with that of previous days. 
```{r} 
Box.test(diff(goog200), lag=10, type="Ljung-Box") 
``` 

Occasionally the differenced data will not appear to be stationary and it may be necessary to difference the data a second time to obtain a stationary series. Then, we would model the “change in the changes” of the original data. In practice, it is almost never necessary to go beyond second-order differences. 

Example: first log, then difference, then difference again. 
```{r} 
cbind("Billion kWh" = usmelec, 
      "Logs" = log(usmelec), 
      "Seasonally\n differenced logs" = 
        diff(log(usmelec),12), 
      "Doubly\n differenced logs" = 
        diff(diff(log(usmelec),12),1)) %>% 
  autoplot(facets=TRUE) + 
    xlab("Year") + ylab("") + 
    ggtitle("Monthly US net electricity generation") 
``` 

How many times do we to differenciate a series? formal test based on kpss in "ndiff()". The test, on the original series (or on the log series if needed to remove the trend), shows that the price of google stock has to be differenciated 1 time before is stationary. 
```{r} 
ndiffs(goog) 
plot(goog) 
plot(diff(goog)) 
``` 

We can do the same for series that are non seasonal stationary by nsdiffs instead of ndiffs. the function tell us how many times we have to differenciate to eliminate also the seasonal component 
```{r} 
usmelec%>%log()%>%nsdiffs() #says one 
usmelec%>%log()%>%ndiffs()%>%nsdiffs() #says zero 

``` 

## Lag operator notations 

## Autoregressive models 
all the parameters $\forall \phi_{t}</1/$ for the process to be stationary 

## Moving averages 
all the parameters $\forall \theta_{t}</1/$ for the process to be invertible into an $AR(infinite)$. The process is stationary by construction. 

## ARIMA(p,d,q) on R by MLE. How many lags? ggAcf() for p and ggPacf()for q. choose based on AIC. 

When fitting an ARIMA model to a set of (non-seasonal) time series data, the following procedure provides a useful general approach. 

```{r} 

FEE$G.... %>% log() %>% ndiffs() 
FEE$G.... %>% log() %>% diff() %>% ndiffs() 

adf.test(diff(log(FEE$G....))) 

#fit <- auto.arima(FEEd$G...., seasonal=FALSE, stepwise=FALSE, approximation=FALSE) 

fit <- arima(FEE$G....,order = c(1,0,0)) 

plot(scale(FEE$G....)) 
lines(scale(fit$residuals), type="l", col="red") 
abline(h=0) 

fit %>% forecast(h=10) %>% autoplot(include=80) 

ggAcf(FEEd$G....) # for p 
ggPacf(FEEd$G....) # for q 

autoplot(fit) 
``` 

ARIMA model algorithm 
  1. Plot the data and identify any unusual observations. 
  2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance. 
  3. If the data are non-stationary, take first differences of the data until the data are stationary. 
  4. Examine the ACF/PACF: Is an ARIMA(p,d,0) or ARIMA(0,d,q) model appropriate? 
  5. Try your chosen model(s), and use the AICc to search for a better model. 
  6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model. 
  7. Once the residuals look like white noise, calculate forecasts. 

```{r} 
#1 
elecequip %>% stl(s.window='periodic') %>% seasadj() -> eeadj 
autoplot(eeadj) 
#2/3 
eeadj %>% diff() %>% ggtsdisplay(main="") 
#4/5 
(fit <- Arima(eeadj, order=c(3,1,1))) 
#6 
checkresiduals(fit) 
#7 
autoplot(forecast(fit)) 
``` 

Plotting the characterstic roots 
```{r} 
autoplot(fit) 
``` 

## Forecasting 
Shows how to do it manually by forward iteration 

Prediction intervals..dont need to know how to compute them 

## Seasonal ARIMA models ARIMA (p,d,q)(P,D,Q) if you have a seasonal component write like 
Arima(order=c(0,1,1), seasonal=c(0,1,1)) 

```{r} 
autoplot(euretail) + ylab("Retail index") + xlab("Year") 
euretail %>% diff(lag=4) %>% ggtsdisplay() 
euretail %>% diff(lag=4) %>% diff() %>% ggtsdisplay() 
euretail %>% 
  Arima(order=c(0,1,1), seasonal=c(0,1,1)) %>% 
  residuals() %>% ggtsdisplay() 
fit3 <- Arima(euretail, order=c(0,1,3), seasonal=c(0,1,1)) # he used the actual series!!!!!! 
checkresiduals(fit3) 
fit3 %>% forecast(h=12) %>% autoplot() 

# or using autoarima 

auto.arima(euretail, stepwise=FALSE, approximation=FALSE) 
``` 

## ARIMA VS EXPONENTIAL SMOOTHING (ETS) 
It is a commonly held myth that ARIMA models are more general than exponential smoothing. While linear exponential smoothing models are all special cases of ARIMA models, the non-linear exponential smoothing models have no equivalent ARIMA counterparts. On the other hand, there are also many ARIMA models that have no exponential smoothing counterparts. In particular, all ETS models are non-stationary, while some ARIMA models are stationary. 

The ETS models with seasonality or non-damped trend or both have two unit roots (i.e., they need two levels of differencing to make them stationary). All other ETS models have one unit root (they need one level of differencing to make them stationary). 

Comparing the two models by custom function; then pass the object into tsCV(). 
In this case ets outperform autoarima. 

```{r} 
fets <- function(x, h){forecast(ets(x), h=h)} 
farima <- function(x,h){forecast(auto.arima(x), h=h)} 

#compute cross validation for both models 
e1 <- tsCV(FEE$G...., fets, h=1) # not on the differenciated series 
e2 <- tsCV(FEE$G...., farima, h=1) 

#find MSE of each model 
mean(e1^2, na.rm=TRUE) #ets is better 
mean(e2^2, na.rm=TRUE) 

FEE$G.... %>% ets() %>% forecast() %>% autoplot() 
``` 

Example with train and test of a long series 
```{r} 
# Consider the qcement data beginning in 1988 
cement <- window(qcement, start=1988) 
# Use 20 years of the data as the training set 
train <- window(cement, end=c(2007,4)) 

(fit.arima <- auto.arima(train)) 
checkresiduals(fit.arima) 

(fit.ets <- ets(train)) 
checkresiduals(fit.ets) 

# Generate forecasts and compare accuracy over the test set 
a1 <- fit.arima %>% forecast(h = 4*(2013-2007)+1) %>% 
  accuracy(qcement) 
a1[,c("RMSE","MAE","MAPE","MASE")] 

a2 <- fit.ets %>% forecast(h = 4*(2013-2007)+1) %>% 
  accuracy(qcement) 
a2[,c("RMSE","MAE","MAPE","MASE")] 

# Generate forecasts from an ETS model 
cement %>% ets() %>% forecast(h=12) %>% autoplot() 

``` 


# **Dynamic Regression Model** 

# Estimation 
The time series models in the previous two chapters allow for the inclusion of information from past observations of a series, but not for the inclusion of other information that may also be relevant. For example, the effects of holidays, competitor activity, changes in the law, the wider economy, or other external variables, may explain some of the historical variation and may lead to more accurate forecasts. On the other hand, the regression models in Chapter 5 allow for the inclusion of a lot of relevant information from predictor variables, but do not allow for the subtle time series dynamics that can be handled with ARIMA models. In this chapter, we consider how to extend ARIMA models in order to allow other information to be included in the models. 

An important consideration when estimating a regression with ARMA errors is that all of the variables in the model must first be stationary. 
We therefore first difference the non-stationary variables in the model. It is often desirable to maintain the form of the relationship between yt and the predictors, and consequently it is common to difference all of the variables if any of them need differencing. 

## Regression with ARIMA errors in R 
The R function Arima() will fit a regression model with ARIMA errors if the argument xreg is used. The order argument specifies the order of the ARIMA error model. If differencing is specified, then the differencing is applied to all variables in the regression model before the model is estimated. For example, the R command 

```{r} 
fit <- Arima(y, xreg=x, order=c(1,1,0)) 
``` 

Notice that the constant term disappears due to the differencing. To include a constant in the differenced model, specify include.drift=TRUE. 

Example on US Personal consumptio and Income. 
We would like to forecast changes in expenditure based on changes in income. we measure the instantaneous effect of the average change of income on the average change of consumption expenditure. 

```{r} 
autoplot(uschange[,1:2], facets=TRUE) + 
  xlab("Year") + ylab("") + 
  ggtitle("Quarterly changes in US consumption and personal income") 
``` 

```{r} 
(fit <- auto.arima(uschange[,"Consumption"], xreg=uschange[,"Income"])) 
``` 

The data are clearly already stationary (as we are considering percentage changes rather than raw expenditure and income), so there is no need for any differencing. 

$$ 
\begin{align} 
y_{t}&=0.6+0.2x_{t}+\eta_{t}\\ 
\eta_{t}&=0.69eta_{t-1}+epsilon_{t}-0.58epsilon_{t-1}+0.2epsilon_{t-2}\\ 
\epsilon_{t}&=NID(0, 0.322)\\ 
\end{align} 
$$ 

We can recover estimates of both the ηt and  εt series using the residuals() function. 

```{r} 
cbind("Regression Errors" = residuals(fit, type="regression"), 
      "ARIMA errors" = residuals(fit, type="innovation")) %>% 
  autoplot(facets=TRUE) 
``` 

It is the ARIMA errors that should resemble a white noise series. 

```{r} 
checkresiduals(fit) 
``` 

## Forecasting 
To forecast using a regression model with ARIMA errors, we need to forecast the regression part of the model and the ARIMA part of the model, and combine the results. As with ordinary regression models, in order to obtain forecasts we first need to forecast the predictors. When the predictors are known into the future (e.g., calendar-related variables such as time, day-of-week, etc.), this is straightforward. But when the predictors are themselves unknown, we must either model them separately, or use assumed future values for each predictor. 

Previous example: 
We will calculate forecasts for the next eight quarters assuming that the future percentage changes in personal disposable income will be equal to the mean percentage change from the last forty years. 
```{r} 
fcast <- forecast(fit, xreg=rep(mean(uschange[,2]),8)) 
autoplot(fcast) + xlab("Year") + 
  ylab("Percentage change") 
``` 

The prediction intervals for this model are narrower than those for the model developed in Section 8.5 because we are now able to explain some of the variation in the data using the income predictor. It is important to realise that the prediction intervals from regression models (with or without ARIMA errors) do not take into account the uncertainty in the forecasts of the predictors. So they should be interpreted as being conditional on the assumed (or estimated) future values of the predictor variables. 

NEW EXAMPLE: FORECASTING ELECTRICITY DEMAND 
Daily electricity demand can be modelled as a function of temperature. As can be observed on an electricity bill, more electricity is used on cold days due to heating and hot days due to air conditioning. The higher demand on cold and hot days is reflected in the u-shape of Figure 9.5, where daily demand is plotted versus daily maximum temperature. 

The data are stored as elecdaily including total daily demand, an indicator variable for workdays (a workday is represented with 1, and a non-workday is represented with 0), and daily maximum temperatures. Because there is weekly seasonality, the frequency has been set to 7. Figure 9.6 shows the time series of both daily demand and daily maximum temperatures. The plots highlight the need for both a non-linear and a dynamic model. 

In this example, we fit a quadratic regression model with ARMA errors using the auto.arima() function. 

```{r} 
xreg <- cbind(MaxTemp = elecdaily[, "Temperature"], 
              MaxTempSq = elecdaily[, "Temperature"]^2, 
              Workday = elecdaily[, "WorkDay"]) 
fit <- auto.arima(elecdaily[, "Demand"], xreg = xreg) #the level of demand as y 
checkresiduals(fit) 
``` 

The model has some significant autocorrelation in the residuals, which means the prediction intervals may not provide accurate coverage. Also, the histogram of the residuals shows one positive outlier, which will also affect the coverage of the prediction intervals. 

Using the estimated model we forecast 14 days ahead starting from Thursday 1 January 2015 (a non-work-day being a public holiday for New Years Day). In this case, we could obtain weather forecasts from the weather bureau for the next 14 days. But for the sake of illustration, we will use scenario based forecasting (as introduced in Section 5.6) where we set the temperature for the next 14 days to a constant 26 degrees. 

```{r} 
fcast <- forecast(fit, 
  xreg = cbind(rep(26,14), rep(26^2,14), 
    c(0,1,0,0,1,1,1,1,1,0,0,1,1,1))) 
autoplot(fcast) + ylab("Electicity demand (GW)") 
``` 

The point forecasts look reasonable for the first two weeks of 2015. The slow down in electricity demand at the end of 2014 (due to many people taking summer vacations) has caused the forecasts for the next two weeks to show similarly low demand values. 


## STOCHASTIC AND DETERMINISTIC TREND 
There are two different ways of modelling a linear trend. 
1 A deterministic trend is obtained using the regression model $y_{t}=\beta_{0}+\beta_{1}t+\eta_{t}$ where  ηt is an ARMA process.
2 A stochastic trend is obtained using the model $y_{t}=\beta_{0}+\beta_{1}t+\eta_{t}$ where ηt is an ARIMA process with  d=1. 

The deterministic trend model is obtained as follows: 

$$ 
\begin{align} 
y_{t}&=0.42+0.17t+\eta_{t}\\ 
\eta_{t}&=1.11\eta_{t-1}-0.38\eta_{t-2}\\ 
\epsilon_{t}&=NID(0, 0.0298)\\ 
\end{align} 
$$ 

```{r} 
trend <- seq_along(austa) 
(fit1 <- auto.arima(austa, d=0, xreg=trend)) 
``` 

Alternatively, the stochastic trend model can be estimated. 

$$ 
\begin{align} 
y_{t}&=y_{0}+0.17t+\eta_{t}\\ 
\eta_{t}&=\eta_{t-1}+0.3\epsilon_{t-1}+\epsilon_{t}\\ 
\epsilon_{t}&=NID(0, 0.338)\\ 
\end{align} 
$$ 

```{r} 
(fit2 <- auto.arima(austa, d=1)) 
``` 

FORECASTS ARE DIFFERENT. 
```{r} 
fc1 <- forecast(fit1, 
  xreg = cbind(trend = length(austa) + 1:10)) 
fc2 <- forecast(fit2, h=10) 
autoplot(austa) + 
  autolayer(fc2, series="Stochastic trend") + 
  autolayer(fc1, series="Deterministic trend") + 
  ggtitle("Forecasts from trend models") + 
  xlab("Year") + ylab("Visitors to Australia (millions)") + 
  guides(colour=guide_legend(title="Forecast")) 
``` 

There is an implicit assumption with deterministic trends that the slope of the trend is not going to change over time. On the other hand, stochastic trends can change, and the estimated growth is only assumed to be the average growth over the historical period, not necessarily the rate of growth that will be observed into the future. Consequently, it is safer to forecast with stochastic trends, especially for longer forecast horizons, as the prediction intervals allow for greater uncertainty in future growth. 

## Dynamic harmonic regression 
When there are long seasonal periods, a dynamic regression with Fourier terms is often better than other models we have considered in this book. For example, daily data can have annual seasonality of length 365, weekly data has seasonal period of approximately 52, while half-hourly data can have several seasonal periods, the shortest of which is the daily pattern of period 48. Seasonal versions of ARIMA and ETS models are designed for shorter periods such as 12 for monthly data or 4 for quarterly data. 

So for such time series, we prefer a harmonic regression approach where the seasonal pattern is modelled using Fourier terms with short-term time series dynamics handled by an ARMA error. The only real disadvantage (compared to a seasonal ARIMA model) is that the seasonality is assumed to be fixed 

it allows any length seasonality; 
1 for data with more than one seasonal period, Fourier terms of different frequencies can be included; 
2 the seasonal pattern is smooth for small values of  K(but more wiggly seasonality can be handled by increasing K); 
3 the short-term dynamics are easily handled with a simple ARMA error. 

```{r} 
cafe04 <- window(auscafe, start=2004) 
plots <- list() 
for (i in seq(6)) { 
  fit <- auto.arima(cafe04, xreg = fourier(cafe04, K = i), 
    seasonal = FALSE, lambda = 0) 
  plots[[i]] <- autoplot(forecast(fit, 
      xreg=fourier(cafe04, K=i, h=24))) + 
    xlab(paste("K=",i,"   AICC=",round(fit[["aicc"]],2))) + 
    ylab("") + ylim(1.5,4.7) 
} 
gridExtra::grid.arrange( 
  plots[[1]],plots[[2]],plots[[3]], 
  plots[[4]],plots[[5]],plots[[6]], nrow=3) 
``` 

## Lagged predictors 
Sometimes, the impact of a predictor which is included in a regression model will not be simple and immediate. For example, an advertising campaign may impact sales for some time beyond the end of the campaign, and sales in one month will depend on the advertising expenditure in each of the past few months. In these situations, we need to allow for lagged effects of the predictor. 

EXAMPLE 
```{r} 
autoplot(insurance, facets=TRUE) + 
  xlab("Year") + ylab("") + 
  ggtitle("Insurance advertising and quotations") 
``` 

We will consider including advertising expenditure for up to four months; that is, the model may include advertising expenditure in the current month, and the three months before that. When comparing models, it is important that they all use the same training set. In the following code, we exclude the first three months in order to make fair comparisons. 

```{r} 
# Lagged predictors. Test 0, 1, 2 or 3 lags. 
Advert <- cbind( 
    AdLag0 = insurance[,"TV.advert"], 
    AdLag1 = stats::lag(insurance[,"TV.advert"],-1), 
    AdLag2 = stats::lag(insurance[,"TV.advert"],-2), 
    AdLag3 = stats::lag(insurance[,"TV.advert"],-3)) %>% 
  head(NROW(insurance)) 

# Restrict data so models use same fitting period 
fit1 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1], 
  stationary=TRUE) 
fit2 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1:2], 
  stationary=TRUE) 
fit3 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1:3], 
  stationary=TRUE) 
fit4 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1:4], 
  stationary=TRUE) 

c(fit1[["aicc"]],fit2[["aicc"]],fit3[["aicc"]],fit4[["aicc"]]) 
``` 

The best model (with the smallest AICc value) has two lagged predictors; that is, it includes advertising only in the current month and the previous month. So we now re-estimate that model, but using all the available data. 

```{r} 
(fit <- auto.arima(insurance[,1], xreg=Advert[,1:2], 
  stationary=TRUE)) 
``` 

The chosen model has AR(3) errors. The model can be written as $ y_{t}=2.04+1.26x_{t}+0.16x_{t-1}+\eta_{t}$ 
where  yt is the number of quotations provided in month  t, xt is the advertising expenditure in month t, 
$\eta_{t}=1.41\eta_{t-1}-0.93\eta_{t-2}+0.36\eta_{t-3}\epsilon_{t}$ and εt is white noise. 

We can calculate forecasts using this model if we assume future values for the advertising variable. If we set the future monthly advertising to 8 units, we get the forecasts in Figure 

```{r} 
fc8 <- forecast(fit, h=20, 
  xreg=cbind(AdLag0 = rep(8,20), 
             AdLag1 = c(Advert[40,1], rep(8,19)))) 
autoplot(fc8) + ylab("Quotes") + 
  ggtitle("Forecast quotes with future advertising set to 8") 
``` 


# **10 FORECASTING HIERARCHICAL OR GROUPED TIME SERIES** 

## Hierarchical time series 

Time series can often be naturally disaggregated by various attributes of interest. 

Hierarchical time series often arise due to geographic divisions. For example, the total bicycle sales can be disaggregated by country, then within each country by state, within each state by region, and so on down to the outlet level. 

It is common to produce disaggregated forecasts based on disaggregated time series, and we usually require the forecasts to add up in the same way as the data. For example, forecasts of regional sales should add up to give forecasts of state sales, which should in turn add up to give a forecast for the national sales. 

In this chapter we discuss forecasting large collections of time series that must add up in some way. The challenge is that we require forecasts that are coherent across the aggregation structure. That is, we require forecasts to add up in a manner that is consistent with the aggregation structure of the collection of time series. 

EXAMPLE: AUSTRALIAN TOURISM HIERARCHY 
Australia is divided into eight geographic areas (some called states and others called territories) with each one having its own government and some economic and administrative autonomy. Each of these can be further subdivided into smaller areas of interest, referred to as zones. Business planners and tourism authorities are interested in forecasts for the whole of Australia, for the states and the territories, and also for the zones. In this example we concentrate on quarterly domestic tourism demand, measured as the number of visitor nights Australians spend away from home. To simplify our analysis, we combine the two territories and Tasmania into an “Other” state. So we have six states: New South Wales (NSW), Queensland (QLD), South Australia (SAU), Victoria (VIC), Western Australia (WAU) and Other (OTH). For each of these we consider visitor nights within the following zones. 

1 NSW        Metro (NSWMetro), North Coast (NSWNthCo), South Coast (NSWSthCo), South Inner (NSWSthIn), North Inner (NSWNthIn) 
2 QLD        Metro (QLDMetro), Central (QLDCntrl), North Coast (QLDNthCo) 
3 SAU        Metro (SAUMetro), Coastal (SAUCoast), Inner (SAUInner) 
4 VIC        Metro (VICMetro), West Coast (VICWstCo), East Coast (VICEstCo), Inner (VICInner) 
5 WAU        Metro (WAUMetro), Coastal (WAUCoast), Inner (WAUInner) 
6 OTH        Metro (OTHMetro), Non-Metro (OTHNoMet) 

We consider five zones for NSW, four zones for VIC, and three zones each for QLD, SAU and WAU. Note that Metro zones contain the capital cities and surrounding areas. 

To create a hierarchical time series, we use the hts() function as shown in the code below. The function requires two inputs: the bottom-level time series and information about the hierarchical structure. visnights is a time series matrix containing the bottom-level series. There are several ways to input the structure of the hierarchy. In this case we are using the characters argument. 
-->The first three characters of each column name of visnights capture the categories at the first level of the hierarchy (States). 
-->The following five characters capture the bottom-level categories (Zones). 

```{r} 
#install.packages("hts") 
library(hts) 
tourism.hts <- hts(visnights, characters = c(3, 5)) 
tourism.hts %>% aggts(levels=0:1) %>% 
  autoplot(facet=TRUE) + 
  xlab("Year") + ylab("millions") + ggtitle("Visitor nights") 
``` 

The top plot in Figure 10.2 shows the total number of visitor nights for the whole of Australia, while the plots below show the data disaggregated by state. These reveal diverse and rich dynamics at the aggregate national level, and the first level of disaggregation for each state. The aggts() function extracts time series from an hts object for any level of aggregation. 

The plots in Figure 10.3 show the bottom-level time series, namely the visitor nights for each zone. These help us visualise the diverse individual dynamics within each zone, and assist in identifying unique and important time series. Notice, for example, the coastal WAU zone which shows significant growth over the last few years. 

```{r} 
#install.packages("tidyverse") 
library(tidyverse) 
cols <- sample(scales::hue_pal(h=c(15,375), 
  c=100,l=65,h.start=0,direction = 1)(NCOL(visnights))) 
as_tibble(visnights) %>% 
  gather(Zone) %>% 
  mutate(Date = rep(time(visnights), NCOL(visnights)), 
         State = str_sub(Zone,1,3)) %>% 
  ggplot(aes(x=Date, y=value, group=Zone, color=Zone)) + 
    geom_line() + 
    facet_grid(State~., scales="free_y") + 
    xlab("Year") + ylab("millions") + 
    ggtitle("Visitor nights by Zone") + 
    scale_color_manual(values = cols) 
``` 

# Grouped time series 

Grouped time series involve more general aggregation structures than hierarchical time series. For example, we could further disaggregate all geographic levels of the Australian tourism data by purpose of travel 

EXAMPLE: Example: Australian prison population 

The top row of Figure 10.5 shows the total number of prisoners in Australia over the period 2005 Q1 to 2016 Q4. This represents the top-level series in the grouping structure. The rest of the panels show the prison population disaggregated by (i) state19 (ii) legal status, whether prisoners have already been sentenced or are in remand waiting for a sentence, and (iii) gender. In this example, the three factors are crossed, but none are nested within the others. 

To create a grouped time series, we use the gts() function. Similar to the hts() function, inputs to the gts() function are the bottom-level time series and information about the grouping structure. prison is a time series matrix containing the bottom-level time series. The information about the grouping structure can be passed in using the characters input. (An alternative is to be more explicit about the labelling of the series and use the groups input.) 

```{r} 
library(hts) 
prison.gts <- gts(prison/1e3, characters = c(3,1,9), 
  gnames = c("State", "Gender", "Legal", 
             "State*Gender", "State*Legal", 
             "State*Gender*Legal")) 
``` 

One way to plot the main groups is as follows. 

```{r} 
prison.gts %>% aggts(level=0:3) %>% autoplot() 
``` 

But with a little more work, we can construct Figure 10.5 using the following code. 

```{r} 
p1 <- prison.gts %>% aggts(level=0) %>% 
  autoplot() + ggtitle("Australian prison population") + 
    xlab("Year") + ylab("Total number of prisoners ('000)") 
groups <- aggts(prison.gts, level=1:3) 
cols <- sample(scales::hue_pal(h=c(15,375), 
          c=100,l=65,h.start=0,direction = 1)(NCOL(groups))) 
p2 <- as_tibble(groups) %>% 
  gather(Series) %>% 
  mutate(Date = rep(time(groups), NCOL(groups)), 
         Group = str_extract(Series, "([A-Za-z ]*)")) %>% 
  ggplot(aes(x=Date, y=value, group=Series, color=Series)) + 
    geom_line() + 
    xlab("Year") + ylab("Number of prisoners ('000)") + 
    scale_color_manual(values = cols) + 
    facet_grid(.~Group, scales="free_y") + 
    scale_x_continuous(breaks=seq(2006,2016,by=2)) + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
gridExtra::grid.arrange(p1, p2, ncol=1) 
``` 

Plots of other group combinations can be obtained similarly. Figure 10.6 shows the Australian prison population disaggregated by all possible combinations of two attributes at a time. The top plot shows the prison population disaggregated by state and legal status, the middle panel shows the disaggregation by state and gender and the bottom panel shows the disaggregation by legal status and gender. 

## BOTTOM UP APPROACH TO FORECAST 

A simple method for generating coherent forecasts is the bottom-up approach. This approach involves first generating forecasts for each series at the bottom-level, and then summing these to produce forecasts for all the series in the structure. 

Forecasts can be produced using the forecast() function applied to objects created by hts() or gts(). The hts package has three in-built options to produce forecasts: ETS models, ARIMA models or random walks; these are controlled by the fmethod argument. It also use several methods for producing coherent forecasts, controlled by the method argument. 

For example, suppose we wanted bottom-up forecasts using ARIMA models applied to the prison data. Then we would use 
```{r} 
forecast(prison.gts, method="bu", fmethod="arima") 
``` 

## TOP DOWN APPROACH 
Top-down approaches only work with strictly hierarchical aggregation structures, and not with grouped structures. They involve first generating forecasts for the Total series yt, and then disaggregating these down the hierarchy. 

1 Average historical proportions: This approach is implemented in the forecast() function by setting method="tdgsa", where tdgsa stands for “top-down Gross-Sohl method A”. 
2 Proportions of the historical averages: This approach is implemented in the forecast() function by setting method="tdgsf", where tdgsa stands for “top-down Gross-Sohl method F”. 

## MIDDLE OUT APPROACH 

The middle-out approach combines bottom-up and top-down approaches. First, a “middle level” is chosen and forecasts are generated for all the series at this level. For the series above the middle level, coherent forecasts are generated using the bottom-up approach by aggregating the “middle-level” forecasts upwards. For the series below the “middle level”, coherent forecasts are generated using a top-down approach by disaggregating the “middle level” forecasts downwards. 

This approach is implemented in the forecast() function by setting method="mo" and by specifying the appropriate middle level via the level argument. For the top-down disaggregation below the middle level, the top-down forecast proportions method is used. 

## MAPPING MATRICES 
All of the methods considered so far can be expressed using a common notation. see next topic for the optimal reconciliation matrix to combine individual forecasts. 

## OPTIMAL RECONCILIATION APPROXIMATION 
Optimal forecast reconciliation will occur if we can find the P matrix which minimises the forecast error of the set of coherent forecasts. We present here a simplified summary of the approach. (WHICH I DONT SHOW HERE) 

EXAMPLE 
We compute the forecasts for the Australian prison population, described in Section 10.2. Using the default arguments for the forecast() function, we compute coherent forecasts by the optimal reconciliation approach with the WLS estimator using variance scaling. 
```{r} 
prisonfc <- forecast(prison.gts) 
``` 

To obtain forecasts for each level of aggregation, we can use the aggts() function. For example, to calculate forecasts for the overall total prison population, and for the one-factor groupings (State, Gender and Legal Status), we use: 

```{r} 
fcsts <- aggts(prisonfc, levels=0:3) 
``` 


A simple plot is obtained using 

```{r} 
groups <- aggts(prison.gts, levels=0:3) 
autoplot(fcsts) + autolayer(groups) 
``` 

A nicer plot is available using the following code. The results are shown in Figure 10.8. The vertical line marks the start of the forecast period. 

```{r} 
prisonfc <- ts(rbind(groups, fcsts), 
  start=start(groups), frequency=4) 
p1 <- autoplot(prisonfc[,"Total"]) + 
  ggtitle("Australian prison population") + 
  xlab("Year") + ylab("Total number of prisoners ('000)") + 
  geom_vline(xintercept=2017) 
cols <- sample(scales::hue_pal(h=c(15,375), 
          c=100,l=65,h.start=0,direction = 1)(NCOL(groups))) 
p2 <- as_tibble(prisonfc[,-1]) %>% 
  gather(Series) %>% 
  mutate(Date = rep(time(prisonfc), NCOL(prisonfc)-1), 
         Group = str_extract(Series, "([A-Za-z ]*)")) %>% 
  ggplot(aes(x=Date, y=value, group=Series, color=Series)) + 
    geom_line() + 
    xlab("Year") + ylab("Number of prisoners ('000)") + 
    scale_color_manual(values = cols) + 
    facet_grid(. ~ Group, scales="free_y") + 
    scale_x_continuous(breaks=seq(2006,2018,by=2)) + 
    theme(axis.text.x = element_text(angle=90, hjust=1)) + 
    geom_vline(xintercept=2017) 
gridExtra::grid.arrange(p1, p2, ncol=1) 
``` 

The accuracy() command is useful for evaluating the forecast accuracy across hierarchical or grouped structures. The following table summarises the accuracy of the bottom-up and the optimal reconciliation approaches, forecasting 2015 Q1 to 2016 Q4 as a test period. 

The results show that the optimal reconciliation approach generates more accurate forecasts especially for the top level. In general, we find that as the optimal reconciliation approach uses information from all levels in the structure, it generates more accurate coherent forecasts than the other traditional alternatives which use limited information. 

Table 10.1: Accuracy of Australian prison population forecasts for different groups of series. 
        
              Bottom-up         Optimal 
            
              MAPE        MASE              MAPE        MASE 
Total                5.32        1.84              3.04        1.05 
State                7.59        1.88              7.57        1.84 
Legal status        6.40        1.76              4.46        1.17 
Gender              8.62        2.68              8.63        2.71 
Bottom              15.82        2.23              14.82        2.14 
All series          12.41        2.16              11.78        2.06 


# **ADVANCED FORECASTING METHODS** 

## COMPLEX SEASONALITY 

## VECTOR AUTOREGRESSIONS 
One limitation of the models that we have considered so far is that they impose a unidirectional relationship — the forecast variable is influenced by the predictor variables, but not vice versa. However, there are many cases where the reverse should also be allowed for — where all variables affect each other. 

Such feedback relationships are allowed for in the vector autoregressive (VAR) framework. In this framework, all variables are treated symmetrically. They are all modelled as if they all influence each other equally. In more formal terminology, all variables are now treated as “endogenous”. To signify this, we now change the notation and write all variables as  ys:  y1,t denotes the tth observation of variable y1, y2,tdenotes the tth observation of variable y2, and so on. 

A VAR model is a generalisation of the univariate autoregressive model for forecasting a vector of time series. It comprises one equation per variable in the system. The right hand side of each equation includes a constant and lags of all of the variables in the system. To keep it simple, we will consider a two variable VAR with one lag. We write a 2-dimensional VAR(1) as: 

$$ \begin{align} 
y_{1,t}&=c_{1}+\phi_{11,1}y_{1,t-1}+\phi_{12,1}y_{2,t-1}+e_{1,t}\\ 
y_{2,t}&=c_{2}+\phi_{21,1}y_{1,t-1}+\phi_{22,1}y_{2,t-1}+e_{2,t}\\ 
\end{align}   $$ 

where $e_{i,t}$ are white noise processes that may be contemporaneously correlated. 
The coefficient $\phi_{ii,l}$  captures the influence of the $l^{th}$ lag of the variable $y_{i}$ on itself, while $\phi_{ij,l}$  captures the influence of the $l^{th}$ lag of variable $y_{j}$ on $y_{i}$. 

If the series are stationary, we forecast them by fitting a VAR to the data directly (known as a “VAR in levels”). If the series are non-stationary, we take differences of the data in order to make them stationary, then fit a VAR model (known as a “VAR in differences”). In both cases, the models are estimated equation by equation using the principle of least squares. For each equation, the parameters are estimated by minimising the sum of squared $e_{i,t}$ values. 

There are two decisions one has to make when using a VAR to forecast, namely how many variables and how many lags should be included in the system. For example, for a VAR with K=5 and p=3  lags, there are 16 coefficients per equation, giving a total of 80 coefficients to be estimated. The more coefficients that need to be estimated, the larger the estimation error entering the forecast. 

VAR models are implemented in the vars package in R. It contains a function VARselect() for selecting the number of lags p using four different information criteria: AIC, HQ, SC OR BIC and FPE. For VAR models, we prefer to use the BIC. 

Despite this, VARs are useful in several contexts: 
1 forecasting a collection of related variables where no explicit interpretation is required; 
2 testing whether one variable is useful in forecasting another (the basis of Granger causality tests); 
3 impulse response analysis, where the response of one variable to a sudden but temporary change in another variable is analysed; 
4 forecast error variance decomposition, where the proportion of the forecast variance of each variable is attributed to the effects of the other variables. 

EXAMPLE: 
```{r} 
#install.packages("vars") 
library(vars) 
VARselect(uschange[,1:2], lag.max=8, type="const")[["selection"]] 
``` 

```{r} 
var1 <- VAR(uschange[,1:2], p=1, type="const") 
serial.test(var1, lags.pt=10, type="PT.asymptotic") #portmanteau test for serial cor 
``` 
```{r} 
var2 <- VAR(uschange[,1:2], p=2, type="const") 
serial.test(var2, lags.pt=10, type="PT.asymptotic") 

var3 <- VAR(uschange[,1:2], p=3, type="const") 
serial.test(var3, lags.pt=10, type="PT.asymptotic") 
``` 


Both a VAR(1) and a VAR(2) have some residual serial correlation, and therefore we fit a VAR(3). The residuals for VAR3 pass the portmanteau test. do not reject the null of non serialcorrelation. 

```{r} 
forecast(var3) %>% 
  autoplot() + xlab("Year") 
``` 


## NEURAL NETWORK MODELS 

A neural network can be thought of as a network of “neurons” which are organised in layers. The predictors (or inputs) form the bottom layer, and the forecasts (or outputs) form the top layer. There may also be intermediate layers containing “hidden neurons”.

The simplest networks contain no hidden layers and are equivalent to linear regressions. The coefficients attached to these predictors are called “weights”. The weights are selected in the neural network framework using a “learning algorithm” that minimises a “cost function” such as the MSE. 

Once we add an intermediate layer with hidden neurons, the neural network becomes non-linear. 
This is known as a multilayer feed-forward network, where each layer of nodes receives inputs from the previous layers. 

The inputs to each node are combined using a weighted linear combination. The result is then modified by a nonlinear function before being output (like sigmoid). This tends to reduce the effect of extreme input values, thus making the network somewhat robust to outliers. 

The parameters are “learned” from the data. The values of the weights are often restricted to prevent them from becoming too large. The parameter that restricts the weights is known as the “decay parameter”, and is often set to be equal to 0.1. 

The weights take random values to begin with, and these are then updated using the observed data. Therefore, the network is usually trained several times using different random starting points, and the results are averaged. 

Neural network autoregression 
With time series data, lagged values of the time series can be used as inputs to a neural network, just as we used lagged values in a linear autoregression model (Chapter 8). We call this a neural network autoregression or NNAR model. 

In this book, we only consider feed-forward networks with one hidden layer, and we use the notation NNAR(p,k) to indicate here are p lagged inputs and k nodes in the hidden layer. 
For example, a NNAR(9,5) model is a neural network with the last nine observations used as inputs for forecasting the output yt, and with five neurons in the hidden layer. A NNAR(p,0) model is equivalent to an ARIMA( p,0,0) model, but without the restrictions on the parameters to ensure stationarity. 

The nnetar() function fits an NNAR(p,P,k) model. If the values of p and P are not specified, they are selected automatically. p is No. lags and P is the saesonal componenet. 

When it comes to forecasting, the network is applied iteratively. For forecasting one step ahead, we simply use the available historical inputs. For forecasting two steps ahead, we use the one-step forecast as an input, along with the historical data. This process proceeds until we have computed all the required forecasts. 

EXAMPLE 
telecommunication companies like to predict sunspot activity in order to plan for any future difficulties. Sunspots follow a cycle of length between 9 and 14 years. forecasts from an NNAR(10,6) are shown for the next 30 years. We have set a Box-Cox transformation with lambda=0 to ensure the forecasts stay positive. 

```{r} 
fit <- nnetar(sunspotarea, lambda=0) 
autoplot(forecast(fit,h=30)) 
``` 

Here, the last 10 observations are used as predictors, and there are 6 neurons in the hidden layer. The cyclicity in the data has been modelled well. We can also see the asymmetry of the cycles has been captured by the model, where the increasing part of the cycle is steeper than the decreasing part of the cycle. This is one difference between a NNAR model and a linear AR model — while linear AR models can model cyclicity, the modelled cycles are always symmetric. 

neural networks are not based on a well-defined stochastic model, and so it is not straightforward to derive prediction intervals for the resultant forecasts. we can still compute prediction intervals using simulation where future sample paths are generated using bootstrapped residuals 

Here is a simulation of 9 possible future sample paths for the sunspot data. Each sample path covers the next 30 years after the observed data. 
```{r} 
sim <- ts(matrix(0, nrow=30L, ncol=9L), 
  start=end(sunspotarea)[1L]+1L) 
for(i in seq(9)) 
  sim[,i] <- simulate(fit, nsim=30L) 
autoplot(sunspotarea) + autolayer(sim) 
``` 

if you do 1000 simulations u obtain the same of the forecast package 
```{r} 
fcast <- forecast(fit, PI=TRUE, h=30) 
autoplot(fcast) 
``` 


## BOOTSTRAPPING NAD BAGGING 

First, the time series is Box-Cox-transformed, and then decomposed into trend, seasonal and remainder components using STL. Then we obtain shuffled versions of the remainder component to get bootstrapped remainder series. Because there may be autocorrelation present in an STL remainder series, we cannot simply use the re-draw procedure  Instead, we use a “blocked bootstrap”, where contiguous sections of the time series are selected at random and joined together. These bootstrapped remainder series are added to the trend and seasonal components, and the Box-Cox transformation is reversed to give variations on the original time series. 

EXAMPLE 
```{r} 
bootseries <- bld.mbb.bootstrap(debitcards, 10) %>% 
  as.data.frame() %>% ts(start=2000, frequency=12) 
autoplot(debitcards) + 
  autolayer(bootseries, colour=TRUE) + 
  autolayer(debitcards, colour=FALSE) + 
  ylab("Bootstrapped series") + guides(colour="none") 
``` 

This type of bootstrapping can be useful in two ways. First it helps us to get a better measure of forecast uncertainty, and second it provides a way of improving our point forecasts using “bagging”. 

Almost all prediction intervals from time series models are too narrow. 
There are at least four sources of uncertainty in forecasting using time series models: 
1 The random error term; 
2 The parameter estimates; 
3 The choice of model for the historical data; 
4 The continuation of the historical data generating process into the future. 

Bagged ETS forecasts 
Another use for these bootstrapped time series is to improve forecast accuracy. If we produce forecasts from each of the additional time series, and average the resulting forecasts, we get better forecasts than if we simply forecast the original time series directly. This is called “bagging” which stands for “bootstrap aggregating”. 

```{r} 
sim <- bld.mbb.bootstrap(debitcards, 10) %>% 
  as.data.frame() %>% 
  ts(frequency=12, start=2000) 
fc <- purrr::map(as.list(sim), 
           function(x){forecast(ets(x))[["mean"]]}) %>% 
      as.data.frame() %>% 
      ts(frequency=12, start=start) 
autoplot(debitcards) + 
  autolayer(sim, colour=TRUE) + 
  autolayer(fc, colour=TRUE) + 
  autolayer(debitcards, colour=FALSE) + 
  ylab("Bootstrapped series") + 
  guides(colour="none") 
``` 

 The whole procedure can be handled with the baggedETS() function. By default, 100 bootstrapped series are used, and the length of the blocks used for obtaining bootstrapped residuals is set to 24 for monthly data. 

```{r} 
etsfc <- debitcards %>% ets() %>% forecast(h=36) 
baggedfc <- debitcards %>% baggedETS() %>% forecast(h=36) 
autoplot(debitcards) + 
  autolayer(baggedfc, series="BaggedETS", PI=FALSE) + 
  autolayer(etsfc, series="ETS", PI=FALSE) + 
  guides(colour=guide_legend(title="Forecasts")) 
``` 

bagging gives better forecasts than just applying ets() directly. Of course, it is slower because a lot more computation is required. 


# ** SOME PRACTICAL FORECAST ISSUES ** 

## WEEKLY, DAILY AND SUB DAILY DATA 

WEEKLY DATA 
Weekly data is difficult to work with because the seasonal period (the number of weeks in a year) is both large and non-integer. The average number of weeks in a year is 52.18. Most of the methods we have considered require the seasonal period to be an integer. Even if we approximate it by 52, most of the methods will not handle such a large seasonal period efficiently. 

The simplest approach is to use an STL decomposition along with a non-seasonal method applied to the seasonally adjusted data (as discussed in Chapter 6). Here is an example using weekly data on US finished motor gasoline products supplied (in millioins of barrels per day) from February 1991 to May 2005. 

```{r} 
gasoline %>% stlf() %>% autoplot() 
``` 

DAILY AND SUB DAILY 

Daily and sub-daily data are challenging for a different reason — they often involve multiple seasonal patterns, and so we need to use a method that handles such complex seasonality. 

Of course, if the time series is relatively short so that only one type of seasonality is present, then it will be possible to use one of the single-seasonal methods we have discussed in previous chapters (e.g., ETS or a seasonal ARIMA model). But when the time series is long enough so that some of the longer seasonal periods become apparent, it will be necessary to use STL, dynamic harmonic regression or TBATS 

However, note that even these models only allow for regular seasonality. 

The best way to deal with moving holiday effects is to use dummy variables. 

Amongst the models discussed in this book (and implemented in the forecast package for R), the only choice is a dynamic regression model, where the predictors include any dummy holiday effects (and possibly also the seasonality using Fourier terms). 

## TIME SERIES OF COUNTS 

## EUNSURING FORECASTS STAY WITHIN LIMITES 

It is common to want forecasts to be positive, or to require them to be within some specified range [a,b]. . Both of these situations are relatively easy to handle using transformations. 

POSITIVE FORECAST 
To impose a positivity constraint, simply work on the log scale, by specifying the Box-Cox parameter $\lambda=0$ 
```{r} 
eggs %>% 
  ets(model="AAN", damped=FALSE, lambda=0) %>% 
  forecast(h=50, biasadj=TRUE) %>% 
  autoplot() 
``` 

FORECAST CONSTRAINED TO AN INTERVAL 
To see how to handle data constrained to an interval, imagine that the egg prices were constrained to lie within a=50 and b=400. Then we can transform the data using a scaled logit transform which maps (a,b) to the whole real line: $y=log\frac{(x-a)}{(b-x)}$, where x is on the original scale and y is the transformed data. To reverse the transformation, we will use $x=\frac{(b-a)e^y}{1+e^y}+a$ which is not a build in function so we need to create it 

```{r} 
  # Bounds 
    a <- 50 
    b <- 400 
    # Transform data and fit model 
    fit <- log((eggs-a)/(b-eggs)) %>% 
      ets(model="AAN", damped=FALSE) 
    fc <- forecast(fit, h=50) 
    # Back-transform forecasts 
    fc[["mean"]] <- (b-a)*exp(fc[["mean"]]) / 
      (1+exp(fc[["mean"]])) + a 
    fc[["lower"]] <- (b-a)*exp(fc[["lower"]]) / 
     (1+exp(fc[["lower"]])) + a 
    fc[["upper"]] <- (b-a)*exp(fc[["upper"]]) / 
     (1+exp(fc[["upper"]])) + a 
    fc[["x"]] <- eggs 
    # Plot result on original scale 
    autoplot(fc) 
``` 

No bias-adjustment has been used here, so the forecasts are the medians of the future distributions. The prediction intervals from these transformations have the same coverage probability as on the transformed scale, because quantiles are preserved under monotonically increasing transformations. 

The prediction intervals lie above 50 due to the transformation. As a result of this artificial (and unrealistic) constraint, the forecast distributions have become extremely skewed. 

## FORECAST COMBINATIONS 

combining multiple forecasts leads to increased forecast accuracy. In many cases one can make dramatic performance improvements by simply averaging the forecasts. While there has been considerable research on using weighted averages, or some other more complicated combination approach, using a simple average has proven hard to beat. 

```{r} 
train <- window(auscafe, end=c(2012,9)) 
h <- length(auscafe) - length(train) 

ETS <- forecast(ets(train), h=h) 

ARIMA <- forecast(auto.arima(train, lambda=0, biasadj=TRUE),h=h) 

STL <- stlf(train, lambda=0, h=h, biasadj=TRUE) 

NNAR <- forecast(nnetar(train), h=h) 

TBATS <- forecast(tbats(train, biasadj=TRUE), h=h) 

Combination <- (ETS[["mean"]] + ARIMA[["mean"]] + 
  STL[["mean"]] + NNAR[["mean"]] + TBATS[["mean"]])/5 

# plot all the models 
autoplot(auscafe) + 
  autolayer(ETS, series="ETS", PI=FALSE) + 
  autolayer(ARIMA, series="ARIMA", PI=FALSE) + 
  autolayer(STL, series="STL", PI=FALSE) + 
  autolayer(NNAR, series="NNAR", PI=FALSE) + 
  autolayer(TBATS, series="TBATS", PI=FALSE) + 
  autolayer(Combination, series="Combination") + 
  xlab("Year") + ylab("$ billion") + 
  ggtitle("Australian monthly expenditure on eating out") 
``` 

```{r} 
c(ETS = accuracy(ETS, auscafe)["Test set","RMSE"], 
  ARIMA = accuracy(ARIMA, auscafe)["Test set","RMSE"], 
  `STL-ETS` = accuracy(STL, auscafe)["Test set","RMSE"], 
  NNAR = accuracy(NNAR, auscafe)["Test set","RMSE"], 
  TBATS = accuracy(TBATS, auscafe)["Test set","RMSE"], 
  Combination = 
    accuracy(Combination, auscafe)["Test set","RMSE"]) 
``` 

TBATS does particularly well with this series, but the combination approach is even better. For other data, TBATS may be quite poor, while the combination approach is almost always close to, or better than, the best component method. 

## PREDICTION INTERVALS FOR AGGREGATES 

we may have monthly data but wish to forecast the total for the next year. Or we may have weekly data, and want to forecast the total for the next four weeks. 

If the point forecasts are means, then adding them up will give a good estimate of the total. But prediction intervals are more tricky due to the correlations between forecast errors. 

A general solution is to use simulations. 

```{r} 
# First fit a model to the data 
fit <- ets(gas/1000) 
# Forecast six months ahead 
fc <- forecast(fit, h=6) 
# Simulate 10000 future sample paths 
nsim <- 10000 
h <- 6 
sim <- numeric(nsim) 
for(i in seq_len(nsim)) 
  sim[i] <- sum(simulate(fit, future=TRUE, nsim=h)) 
meanagg <- mean(sim) 
``` 

The mean of the simulations is close to the sum of the individual forecasts: 

```{r} 
sum(fc[["mean"]][1:6]) 
meanagg 
``` 

Prediction intervals are also easy to obtain: 

```{r} 
#80% interval: 
quantile(sim, prob=c(0.1, 0.9)) 

#95% interval: 
quantile(sim, prob=c(0.025, 0.975)) 
``` 

## BACKTESTING 

Sometimes it is useful to “backcast” a time series — that is, forecast in reverse time.  Although there are no in-built R functions to do this, it is easy to implement. The following functions reverse a ts object and a forecast object. 

```{r} 
# Function to reverse time 
reverse_ts <- function(y) 
{ 
  ts(rev(y), start=tsp(y)[1L], frequency=frequency(y)) 
} 
# Function to reverse a forecast 
reverse_forecast <- function(object) 
{ 
  h <- length(object[["mean"]]) 
  f <- frequency(object[["mean"]]) 
  object[["x"]] <- reverse_ts(object[["x"]]) 
  object[["mean"]] <- ts(rev(object[["mean"]]), 
    end=tsp(object[["x"]])[1L]-1/f, frequency=f) 
  object[["lower"]] <- object[["lower"]][h:1L,] 
  object[["upper"]] <- object[["upper"]][h:1L,] 
  return(object) 
} 
``` 

Then we can apply these functions to backcast any time series. Here is an example applied to quarterly retail trade in the Euro area. The data are from 1996 to 2011. We backcast to predict the years 1994-1995. 

```{r} 
# Backcast example 
euretail %>% 
  reverse_ts() %>% 
  auto.arima() %>% 
  forecast() %>% 
  reverse_forecast() -> bc 
autoplot(bc) + 
  ggtitle(paste("Backcasts from",bc[["method"]])) 
``` 


## FORECASTING VERY SHORT TIME SERIES 

We often get asked how few data points can be used to fit a time series model. As with almost all sample size questions, there is no easy answer. It depends on the number of model parameters to be estimated and the amount of randomness in the data. The sample size required increases with the number of parameters to be estimated, and the amount of noise in the data. Some textbooks provide rules-of-thumb giving minimum sample sizes for various time series models. These are misleading and unsubstantiated in theory or practice. Further, they ignore the underlying variability of the data and often overlook the number of parameters to be estimated as well. There is, for example, no justification whatever for the magic number of 30 often given as a minimum for ARIMA modelling. The only theoretical limit is that we need more observations than there are parameters in our forecasting model. However, in practice, we usually need substantially more observations than that. 

Ideally, we would test if our chosen model performs well out-of-sample compared to some simpler approaches. However, with short series, there is not enough data to allow some observations to be withheld for testing purposes, and even time series cross validation can be difficult to apply. The AICc is particularly useful here, because it is a proxy for the one-step forecast out-of-sample MSE. Choosing the model with the minimum AICc value allows both the number of parameters and the amount of noise to be taken into account. 

What tends to happen with short series is that the AIC suggests simple models because anything with more than one or two parameters will produce poor forecasts due to the estimation error. We applied the auto.arima() function to all the series from the M-competition with fewer than 20 observations. There were a total of 144 series, of which 54 had models with zero parameters (white noise and random walks), 73 had models with one parameter, 15 had models with two parameters and 2 series had models with three parameters. Interested readers can carry out the same exercise using the following code. 

```{r} 
#install.packages("Mcomp") 
#install.packages("purrr") 
library(Mcomp) 
library(purrr) 
n <- map_int(M1, function(x) {length(x[["x"]])}) 
M1[n < 20] %>% 
  map_int(function(u) { 
    u[["x"]] %>% 
      auto.arima() %>% 
      coefficients() %>% 
      length() 
  }) %>% 
  table() 
``` 

## FORECASTING VERY LONG TIME SERIES 

Most time series models do not work well for very long time series. The problem is that real data do not come from the models we use. When the number of observations is not large (say up to about 200) the models often work well as an approximation to whatever process generated the data. 

An additional problem is that the optimization of the parameters becomes more time consuming because of the number of observations involved. 

ETS models are designed to handle this situation by allowing the trend and seasonal terms to evolve over time. 
ARIMA models with differencing have a similar property. But dynamic regression models do not allow any evolution of model components. 

## FORECASTING ON TRAINING AND TEST SETS 

```{r} 
library(fpp2) 
training <- subset(auscafe, end=length(auscafe)-61) 
test <- subset(auscafe, start=length(auscafe)-60) 
cafe.train <- Arima(training, order=c(2,1,1), 
  seasonal=c(0,1,2), lambda=0) 
cafe.train %>% 
  forecast(h=60) %>% 
  autoplot() + autolayer(test) 
``` 

The fitted() function has an h argument to allow for h-step “fitted values” on the training set. Figure 12.8 is a plot of 12-step (one year) forecasts on the training set. Because the model involves both seasonal (lag 12) and first (lag 1) differencing, it is not possible to compute these forecasts for the first few observations. 

```{r} 
autoplot(training, series="Training data") + 
  autolayer(fitted(cafe.train, h=12), 
    series="12-step fitted values") 
``` 

## MISSING VALUES 

and missing values occur on public holidays when the store is closed. The following day may have increased sales as a result. If we fail to allow for this in our forecasting model, we will most likely under-estimate sales on the first day after the public holiday, but over-estimate sales on the days after that. In other situations, the missingness may be essentially random. For example, someone may have forgotten to record the sales figures, or the data recording device may have malfunctioned. If the timing of the missing data is not informative for the forecasting problem, then the missing values can be handled more easily. The R functions for ARIMA models, dynamic regression models and NNAR models will also work correctly without causing errors. However, other modelling functions do not handle missing values including ets(), stlf(), and tbats(). 

Alternatively, we could replace the missing values with estimates. The na.interp() function is designed for this purpose. 

The gold data contains daily morning gold prices from 1 January 1985 to 31 March 1989. This series was provided to us as part of a consulting project; it contains 34 missing values as well as one apparently incorrect value. Figure 12.9 shows estimates of the missing observations in red. 

```{r} 
gold2 <- na.interp(gold) 
autoplot(gold2, series="Interpolated") + 
  autolayer(gold, series="Original") + 
  scale_color_manual( 
    values=c(`Interpolated`="red",`Original`="gray")) 
``` 

For non-seasonal data like this, simple linear interpolation is used to fill in the missing sections. For seasonal data, an STL decomposition is used estimate the seasonally component, and the seasonally adjusted series are linear interpolated. More sophisticated missing value interpolation is provided in the imputeTS package. 

OUTLIERS 

The tsoutliers() function is designed to identify outliers, and to suggest potential replacement values. In the gold data shown in Figure 12.9, there is an apparently outlier on day 770: 

```{r} 
tsoutliers(gold) 
``` 

Another useful function is tsclean() which identifies and replaces outliers, and also replaces missing values. Obviously this should be used with some caution, but it does allow us to use forecasting models that are sensitive to outliers, or which do not handle missing values. For example, we could use the ets() function on the gold series, after applying tsclean(). 

```{r} 
gold %>% 
  tsclean() %>% 
  ets() %>% 
  forecast(h=50) %>% 
  autoplot() 
``` 

